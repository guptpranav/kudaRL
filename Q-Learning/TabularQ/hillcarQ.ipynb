{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL for MountainCar @ OpenAI gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing requisite modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialising gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: 3\n",
      "Observation Space: [-1.2  -0.07] -> [0.6  0.07]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "print('Action Space:', env.action_space.n)\n",
    "print('Observation Space:', env.observation_space.low, '->', env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TABULAR Q-LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialising Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table size: (20, 20, 3)\n"
     ]
    }
   ],
   "source": [
    "N_x = 20\n",
    "N_v = 20\n",
    "Qtable = np.zeros((N_x, N_v, env.action_space.n))\n",
    "print('Q-table size:', Qtable.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discretised Q-table index of continuously distributed observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_state(state):\n",
    "    index = (state-env.observation_space.low)//((env.observation_space.high-env.observation_space.low)/(N_x,N_v))\n",
    "    return tuple(index.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 30_000\n",
    "rewardLog = np.zeros(EPISODES)\n",
    "\n",
    "ALPHA    = 0.18\n",
    "GAMMA    = 0.99\n",
    "\n",
    "EPS_MAX = 0.90\n",
    "EPS_MIN = 0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(s, a, r, obs):\n",
    "    Qtable[index_state(s)][a] += ALPHA*(r + GAMMA*np.max(Qtable[index_state(obs)]) - Qtable[index_state(s)][a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep: 0:   0%|                               | 0/30000 [00:00<?, ?it/s]/home/pranav/Workspace/RL/deepRL/lib/python3.12/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.goal_position to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.goal_position` for environment variables or `env.get_wrapper_attr('goal_position')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "ep: 0: 100%|██████████████████| 30000/30000 [04:47<00:00, 104.33it/s]\n"
     ]
    }
   ],
   "source": [
    "episode = 0\n",
    "for episode in tqdm(range(EPISODES), desc=f\"ep: {episode}\", leave=True, ncols=69):\n",
    "    done = False\n",
    "    state, info = env.reset(seed = 42)\n",
    "\n",
    "    # exploration rate decay\n",
    "    EPSILON = EPS_MAX - (EPS_MAX - EPS_MIN)*(episode/EPISODES)\n",
    "\n",
    "    while not done:\n",
    "        # epsilon-greedy agent\n",
    "        if np.random.uniform() < EPSILON:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Qtable[index_state(state)])\n",
    "        \n",
    "        # take action\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Q-learning\n",
    "        if not done:\n",
    "            train(state, action, reward, new_state)\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            # print('success in episode', episode)\n",
    "            Qtable[index_state(state)][action] = 0\n",
    "        \n",
    "        # updates\n",
    "        state = new_state\n",
    "        rewardLog[episode] += reward\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 -200.0\n",
      "6000 -200.0\n",
      "9000 -200.0\n",
      "12000 -200.0\n",
      "15000 -200.0\n",
      "18000 -199.86166666666668\n",
      "21000 -198.55633333333333\n",
      "24000 -194.75733333333332\n",
      "27000 -189.47666666666666\n",
      "30000 -181.57933333333332\n"
     ]
    }
   ],
   "source": [
    "N = EPISODES // 10\n",
    "\n",
    "for k in range(EPISODES//N):\n",
    "    print(N*(k+1), np.mean(rewardLog[N*k:N*(k+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = 5\n",
    "test = gym.make(\"MountainCar-v0\", render_mode='human')\n",
    "\n",
    "for k in range(TEST):\n",
    "    done = False\n",
    "    state, info = test.reset(seed = 69)\n",
    "\n",
    "    while not done:\n",
    "        # greedy agent\n",
    "        action = np.argmax(Qtable[index_state(state)])\n",
    "        \n",
    "        # take action\n",
    "        state, reward, terminated, truncated, info = test.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "test.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
