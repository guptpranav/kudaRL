{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI ARMED BANDITS IN PYTHON\n",
    "\n",
    "**epsilon-greedy action value method for solving the k-armed Bandit problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandits:\n",
    "    \"\"\"\n",
    "    numBandits = number of bandits\n",
    "    maxMean    = bandit value cap\n",
    "    maxDev     = bandit standard deviation cap\n",
    "    steps      = total rounds\n",
    "    \"\"\"\n",
    "    def __init__(self, numBandits, maxMean, maxDev, steps):\n",
    "        self.numBandits = numBandits\n",
    "        self.maxMean    = maxMean\n",
    "        self.maxDev     = maxDev\n",
    "        self.steps      = steps\n",
    "\n",
    "        # initialising explore and exploit counters to 0\n",
    "        self.explore, self.exploit = 0, 0\n",
    "\n",
    "        # initialising bandits\n",
    "        self.initBandits()\n",
    "    \n",
    "    # randomly initialises bandit parameters within range with uniform distribution\n",
    "    def initBandits(self):\n",
    "        self.means      = np.random.rand(self.numBandits)*self.maxMean  # bandit value\n",
    "        self.stdevs     = np.random.rand(self.numBandits)*self.maxDev   # bandit standard deviation\n",
    "        self.estValInit = np.random.rand(self.numBandits)*self.maxMean  # initial bandit value estimate\n",
    "    \n",
    "    def simulate(self, epsilon):\n",
    "        # resetting simulation parameters\n",
    "        k, agent = 0, 0\n",
    "        self.explore, self.exploit = 0, 0\n",
    "        self.reward = np.zeros(self.steps)        # step-wise reward \n",
    "        self.chosen = np.zeros(self.numBandits)   # bandit choice counter\n",
    "        self.estVal = np.array(self.estValInit)   # common initial values between runs\n",
    "        \n",
    "        for k in range(self.steps):\n",
    "            greed = np.random.rand()\n",
    "            \n",
    "            if greed <= epsilon:\n",
    "                self.explore += 1\n",
    "                agent = np.random.randint(low=0, high=self.numBandits)   # chose random bandit\n",
    "            else:\n",
    "                self.exploit += 1\n",
    "                agent = np.argmax(self.estVal)     # chose bandit with highest estimated value\n",
    "            \n",
    "            self.reward[k] = np.random.normal(loc=self.means[agent], scale=self.stdevs[agent])  # generate step reward\n",
    "            self.chosen[agent] += 1                                                             # increment bandit counter\n",
    "            self.estVal[agent] += (self.reward[k] - self.estVal[agent])/(self.chosen[agent])    # update estimated value of bandit\n",
    "    \n",
    "    def results(self, epsilon):\n",
    "        self.simulate(epsilon)\n",
    "        \n",
    "        print(\"Simulation Results for ε =\", epsilon, \"greedy\", self.numBandits, \"bandits\", \"running for\", self.steps, \"steps :=\")\n",
    "        for k in range(self.numBandits):\n",
    "            print(\"Bandit\", k+1, \":=\",\n",
    "                  \"est. mean =\", self.estVal[k],\n",
    "                  \"\\tact. mean =\", self.means[k],\n",
    "                  \"\\tstdev =\", self.stdevs[k],\n",
    "                  \"\\tchosen\", int(self.chosen[k]), \"times\")\n",
    "        print(\"explore counter =\", self.explore)\n",
    "        print(\"exploit counter =\", self.exploit)\n",
    "        print(\"total reward =\", np.sum(self.reward))\n",
    "        print(\"regret =\", np.max(self.means)*self.steps)\n",
    "        print(\"% regret =\", 100*(1-np.sum(self.reward)/(np.max(self.means)*self.steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating Bandit object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = Bandits(numBandits=5, maxMean=10, maxDev=3, steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running game with the same bandits for different values of epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation Results for ε = 0.1 greedy 5 bandits running for 1000 steps :=\n",
      "Bandit 1 := est. mean = 7.94266628744492 \tact. mean = 8.120189756046312 \tstdev = 2.241590169669475 \tchosen 21 times\n",
      "Bandit 2 := est. mean = 8.197547077701651 \tact. mean = 8.134411592018962 \tstdev = 0.6804241104530381 \tchosen 882 times\n",
      "Bandit 3 := est. mean = 3.426018356053969 \tact. mean = 2.8791345646961766 \tstdev = 1.30273168340194 \tchosen 8 times\n",
      "Bandit 4 := est. mean = 1.9678802116751855 \tact. mean = 1.0072114483586947 \tstdev = 2.779074488092154 \tchosen 27 times\n",
      "Bandit 5 := est. mean = 8.136792157901455 \tact. mean = 8.047281234129215 \tstdev = 1.845343216892775 \tchosen 62 times\n",
      "explore counter = 92\n",
      "exploit counter = 908\n",
      "total reward = 7982.054540922757\n",
      "regret = 8134.4115920189615\n",
      "% regret = 1.8729941234555736\n"
     ]
    }
   ],
   "source": [
    "bandit.results(epsilon=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
